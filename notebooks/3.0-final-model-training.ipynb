{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c067b3",
   "metadata": {},
   "source": [
    "# 3.0 - Final Model Training (HuggingFace)\n",
    "\n",
    "Fine-tune a transformer model on combined processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - imports & config load\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "CONFIG = Path('configs/training_config.yaml')\n",
    "if CONFIG.exists():\n",
    "    cfg = yaml.safe_load(open(CONFIG))\n",
    "else:\n",
    "    cfg = {'model':{'hf_model':'bert-base-multilingual-cased','max_length':256}, 'data':{'test_size':0.1}, 'training_args':{'num_train_epochs':1,'per_device_train_batch_size':8,'per_device_eval_batch_size':8,'evaluation_strategy':'epoch'}}\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - dataset assembly helper\n",
    "from glob import glob\n",
    "\n",
    "def load_processed_data(folder='data/processed'):\n",
    "    files = glob(folder + '/*.csv')\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        if 'clean_text' not in df.columns:\n",
    "            text_cols = [c for c in df.columns if 'text' in c.lower() or 'content' in c.lower()]\n",
    "            if text_cols:\n",
    "                df['clean_text'] = df[text_cols[0]].astype(str)\n",
    "            else:\n",
    "                continue\n",
    "        if 'label' not in df.columns:\n",
    "            continue\n",
    "        dfs.append(df[['clean_text','label']])\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=['clean_text','label'])\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "full = load_processed_data()\n",
    "print('combined rows', len(full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - dataset to HF dataset\n",
    "from datasets import Dataset\n",
    "if len(full) == 0:\n",
    "    print(\"No processed data found. Skipping HF training cells.\")\n",
    "else:\n",
    "    full = full.dropna()\n",
    "    hf = Dataset.from_pandas(full)\n",
    "    hf = hf.train_test_split(test_size=cfg['data'].get('test_size', 0.1))\n",
    "    print(\"HF dataset prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - tokenizer & model prep\n",
    "if len(full) > 0:\n",
    "    model_name = cfg['model']['hf_model']\n",
    "    num_labels = len(sorted(full['label'].unique()))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['clean_text'], truncation=True, padding='max_length', max_length=cfg['model']['max_length'])\n",
    "\n",
    "    hf = hf.map(tokenize, batched=True)\n",
    "    hf = hf.remove_columns(['clean_text'])\n",
    "    print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - training (lightweight)\n",
    "if len(full) > 0:\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    training_args = TrainingArguments(**cfg['training_args'])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        import numpy as np\n",
    "        preds, labels = eval_pred\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'f1': f1_score(labels, preds, average='weighted')\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=hf['train'], eval_dataset=hf['test'], tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "    trainer.save_model('models/final/transformer_model')\n",
    "    print('Saved transformer model to models/final/transformer_model')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
